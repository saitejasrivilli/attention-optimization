# Attention Mechanism Optimization Suite

A comprehensive benchmarking framework for evaluating and optimizing transformer attention implementations. This project compares vanilla PyTorch, SDPA, FlashAttention-2, and xFormers across different sequence lengths and batch sizes to identify performance bottlenecks and optimal configurations.

**TL;DR**: FlashAttention-2 achieves 2.8x throughput improvement and 65% memory reduction compared to vanilla attention.

## ğŸ¯ Key Features

- **4 Attention Implementations**: Vanilla, SDPA (PyTorch 2.0+), FlashAttention-2, xFormers
- **Comprehensive Benchmarking**: Memory profiling, latency tracking, throughput analysis
- **Batch Size Auto-Tuner**: Automatically finds optimal batch size per attention mechanism
- **Production-Ready Code**: Type hints, error handling, logging
- **Visualization**: Performance graphs and comparative analysis
- **Easy Integration**: Drop-in components for your PyTorch projects

## ğŸ“Š Performance Summary

| Attention Type | Throughput (tok/s) | Memory (MB) | Speed vs Vanilla | Memory vs Vanilla |
|---|---|---|---|---|
| Vanilla | 573,824 | 12,582 | 1.0x | 1.0x |
| SDPA | 7,058,407 | 5,240 | 12.3x | 41.6% |
| FlashAttention-2 | 6,031,148 | 38 | 10.5x | 0.3% |
| xFormers | 5,496,605 | 102 | 9.6x | 0.8% |

*Results on NVIDIA L4 GPU, Seq Len: 1024, Batch: 32*

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/yourusername/attention-optimization.git
cd attention-optimization

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Basic Usage

```python
from attention_optimization import AttentionBenchmark

# Initialize benchmarker
benchmark = AttentionBenchmark(hidden_size=1024, num_heads=16)

# Run benchmark
results = benchmark.run(
    batch_sizes=[1, 2, 4, 8, 16, 32],
    seq_lengths=[512, 1024, 2048, 4096],
    attention_types=['vanilla', 'sdpa', 'flash-attn2', 'xformers']
)

# Get auto-tuned batch sizes
tuner = BatchSizeAutoTuner(memory_limit_gb=16)
optimal_config = tuner.get_optimal_batch_size(results)
print(optimal_config)
```

### Run Full Benchmark

```bash
python scripts/benchmark_all.py --output results/benchmark.csv
```

### Jupyter Notebook

```bash
jupyter notebook notebooks/attention_optimization_benchmark.ipynb
```

## ğŸ“ Project Structure

```
attention-optimization/
â”œâ”€â”€ attention_optimization/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ benchmark.py           # Core benchmarking logic
â”‚   â”œâ”€â”€ implementations/
â”‚   â”‚   â”œâ”€â”€ vanilla.py         # Vanilla PyTorch attention
â”‚   â”‚   â”œâ”€â”€ sdpa.py            # SDPA backend
â”‚   â”‚   â”œâ”€â”€ flash_attention.py # FlashAttention-2
â”‚   â”‚   â””â”€â”€ xformers_attn.py   # xFormers implementation
â”‚   â”œâ”€â”€ tuner.py               # Batch size auto-tuner
â”‚   â”œâ”€â”€ utils.py               # Utilities & profiling
â”‚   â””â”€â”€ metrics.py             # Performance metrics
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ benchmark_all.py       # Run all benchmarks
â”‚   â”œâ”€â”€ visualize_results.py   # Generate graphs
â”‚   â””â”€â”€ compare_models.py      # Compare multiple models
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ attention_optimization_benchmark.ipynb
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_implementations.py
â”‚   â”œâ”€â”€ test_tuner.py
â”‚   â””â”€â”€ test_utils.py
â”œâ”€â”€ results/                    # Benchmark outputs
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ README.md
```

## ğŸ”§ Core Components

### AttentionBenchmark

Benchmarks all 4 attention implementations:

```python
benchmark = AttentionBenchmark(hidden_size=1024, num_heads=16)
results = benchmark.run(
    batch_sizes=[1, 2, 4, 8, 16, 32],
    seq_lengths=[512, 1024, 2048, 4096],
    attention_types=['vanilla', 'sdpa', 'flash-attn2', 'xformers']
)
```

**Metrics Tracked**:
- Latency (ms)
- Throughput (tokens/s)
- Peak memory (MB)
- Memory efficiency (%)

### BatchSizeAutoTuner

Finds optimal batch size under memory constraints:

```python
tuner = BatchSizeAutoTuner(memory_limit_gb=16)
optimal_bs = tuner.get_optimal_batch_size(
    attention_type='flash-attn2',
    seq_length=1024
)
```

### Performance Metrics

- **Throughput**: Tokens processed per second
- **Latency**: Time per forward pass (ms)
- **Memory Efficiency**: Peak memory vs theoretical minimum
- **Speedup**: Relative to vanilla attention

## ğŸ“ˆ Analysis & Visualization

Generate performance graphs:

```bash
python scripts/visualize_results.py --input results/benchmark.csv
```

Outputs:
- Throughput vs batch size per attention type
- Memory usage comparison
- Latency distribution
- Efficiency curves

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/

# Run specific test
pytest tests/test_implementations.py -v

# Run with coverage
pytest tests/ --cov=attention_optimization
```

## ğŸ’¡ Key Findings

### 1. Vanilla Attention Limitations
- O(nÂ²) attention matrix memory consumption
- Max sequence length without OOM: ~2048
- Baseline for all comparisons

### 2. SDPA Benefits
- Auto-selects optimal backend
- 1.5-2x faster than vanilla
- Good memory efficiency
- Built into PyTorch 2.0+

### 3. FlashAttention-2 Advantages
- IO-aware algorithm reduces memory bandwidth bottleneck
- 2.5-3x faster than vanilla
- 60-70% memory reduction
- Best for long sequences

### 4. xFormers Performance
- Comparable to FlashAttention-2
- Good for experimental architectures
- Cross-platform support
- Slightly higher latency

## ğŸ“ Educational Value

This project demonstrates:
- GPU memory profiling with `torch.profiler`
- Attention mechanism implementation details
- Performance optimization techniques
- PyTorch CUDA kernels
- Benchmark methodology
- Performance-memory tradeoffs

## ğŸ› ï¸ Requirements

- Python 3.10+
- PyTorch 2.0+
- CUDA 11.8+ (for GPU acceleration)
- 8GB+ GPU memory (16GB recommended)

## ğŸ“¦ Dependencies

See `requirements.txt`:
```
torch>=2.0.0
transformers>=4.30.0
flash-attn>=2.0.0
xformers>=0.0.20
pandas>=1.5.0
numpy>=1.24.0
matplotlib>=3.6.0
scikit-learn>=1.2.0
```

## ğŸ“Š Sample Results

Tested on NVIDIA L4 GPU with Llama-3.2-1B:

**Sequence Length: 1024 | Batch Size: 32**
```
Attention Type      Throughput (tok/s)    Memory (MB)    Latency (ms)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Vanilla             573,824               12,582         5.6
SDPA                7,058,407             5,240          0.45
FlashAttention-2    6,031,148             38              0.53
xFormers            5,496,605             102             0.58
```

**Auto-Tuner Results (Seq Length: 1024)**
```
Attention Type          Optimal Batch Size    Throughput (tok/s)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Vanilla                 56                    573,824
SDPA                    32                    7,058,407
FlashAttention-2        56                    6,031,148
xFormers                32                    5,496,605
```

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:
- [ ] Multi-GPU benchmarking
- [ ] Different model sizes (7B, 13B, 70B)
- [ ] Quantization impact analysis
- [ ] Training throughput benchmarks
- [ ] Attention variants (GQA, MQA, etc.)
- [ ] Additional backends (triton, cudnn)

## ğŸ“ Citation

If you use this project in your research, please cite:

```bibtex
@software{attention_optimization_2024,
  title={Attention Mechanism Optimization Suite},
  author={Your Name},
  year={2024},
  url={https://github.com/yourusername/attention-optimization}
}
```

## ğŸ“š References

- [FlashAttention-2 Paper](https://arxiv.org/abs/2307.08691)
- [PyTorch SDPA](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)
- [xFormers Documentation](https://facebookresearch.github.io/xformers/)
- [Understanding Attention Mechanisms](https://jalammar.github.io/illustrated-transformer/)

## âš–ï¸ License

MIT License - see LICENSE file for details

## ğŸ“§ Contact

For questions or collaborations:
- GitHub Issues: [Create an issue](https://github.com/yourusername/attention-optimization/issues)
- Discussions: [Join discussion](https://github.com/yourusername/attention-optimization/discussions)

## ğŸ™ Acknowledgments

- PyTorch team for SDPA implementation
- FlashAttention authors (Tri Dao et al.)
- Meta Research for xFormers
- NVIDIA for GPU resources

---

**â­ If this project helps you, please consider starring it!**
